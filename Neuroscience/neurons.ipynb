{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1397cd",
   "metadata": {},
   "source": [
    "# Neuron Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84162aed",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "The perceptron is a binary neuron that has an intant return represented as a threshold dot product. The boundary function $ w \\cdot x+b = 0 $ is used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1505d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(w, x, b=0.0):\n",
    "  \"\"\"\n",
    "  Binary Threshold Unit\n",
    "  \n",
    "  :param w: weight vector, parameters, how much each feature matters\n",
    "  :param x: input vector, features, the data you feed in\n",
    "  :param b: bias term, offset, constant added to move decision boundary\n",
    "\n",
    "  Examples:\n",
    "        >>> perceptron([1, 1], [0.2, 0.3])\n",
    "        1\n",
    "        >>> perceptron([1, 1], [-0.2, -0.3])\n",
    "        0\n",
    "  \"\"\"\n",
    "  return 1 if (np.dot(w, x) + b) > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8d5b2",
   "metadata": {},
   "source": [
    "## Rate Neuron\n",
    "\n",
    "Each neuron is a \"function\" that computes a weighted sum then apply nonlinearity. Without linearity then stacking neurons is pointless, all the layers collapse into one and the network is only as powerful as a linear function (hyperplane, affine map).\n",
    "$$ y = W_2(W_1x) = W_2W_1x $$\n",
    "Common choices for nonlinearity are: ReLU, tanh, sigmoid, softplus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05994eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_neuron(w, x, b=0.0, nonlinearity=\"relu\"):\n",
    "    \"\"\"\n",
    "    Rate Neuron\n",
    "    \n",
    "    :param w: weight vector, parameters, how much each feature matters\n",
    "    :param x: input vector, features, the data you feed in\n",
    "    :param b: bias term, offset, constant added to move decision boundary\n",
    "    :param nonlinearity: Description\n",
    "    \"\"\"\n",
    "    z = float(np.dot(w, x) + b)\n",
    "    if nonlinearity == \"relu\":\n",
    "        # gate negative inputs\n",
    "        return np.maximum(0.0, z)\n",
    "    if nonlinearity == \"tanh\":\n",
    "        # bound in [-1,1], good for stable dynamics in reservoir/recurrent networks.\n",
    "        return np.tanh(z)\n",
    "    if nonlinearity == \"sigmoid\":\n",
    "        # bound in [0,1], good for probability like outputs or on/off.\n",
    "        return 1.0 / (1.0 + np.power(np.e, z))\n",
    "    if nonlinearity == \"softplus\":\n",
    "        # smooth version of relu\n",
    "        return np.log(1.0 + np.power(np.e, z))\n",
    "    raise ValueError(\"Unknown nonlinearity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e84fb",
   "metadata": {},
   "source": [
    "## Leaky Rate Neuron\n",
    "The previous defined rate neuron still has no time or memory, the leaky neuron adds state (memory).\n",
    "$$ x[t] = x[t-1] + (dt/tau) * (-x[t-1] + I[t-1]) $$\n",
    "Here the input x is the internal state of the neuron and decays each timestep. Input to the neuron pushes it up/down. The data class below shows a leakyrate neuron that can be used in a network. External inputs drive pulses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de33bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(0.0, z)\n",
    "\n",
    "@dataclass\n",
    "class LeakyRateNeuron:\n",
    "    \"\"\"Single leaky rate neuron (leaky integrator + static nonlinearity).\n",
    "\n",
    "    This model maintains an internal state x(t) that integrates input I(t) over time\n",
    "    while exponentially decaying (\"leaking\") toward 0.\n",
    "\n",
    "    Continuous-time form:\n",
    "        dx/dt = (-x + I(t)) / tau\n",
    "        r(t)  = f(gain * x(t) + bias)\n",
    "\n",
    "    Discrete-time Euler update used here:\n",
    "        x <- x + (dt/tau) * (-x + I_in)\n",
    "        r <- f(gain * x + bias)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dt : float\n",
    "        Simulation time step in seconds.\n",
    "    tau : float\n",
    "        Leak time constant in seconds. Larger tau => slower dynamics / more memory.\n",
    "    gain : float\n",
    "        Scales the state before applying the nonlinearity.\n",
    "    bias : float\n",
    "        Additive offset before applying the nonlinearity.\n",
    "    f : callable\n",
    "        Nonlinearity mapping an array-like input to array-like output.\n",
    "        Common choices: ReLU, tanh, sigmoid, softplus.\n",
    "    x : float\n",
    "        Internal state (memory) of the neuron.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This class is network-ready: the input to `step(I_in)` is provided externally,\n",
    "    so I_in can come from synapses (W @ r), an external stimulus, noise, etc.\n",
    "    \"\"\"\n",
    "    dt: float = field(default=1e-3)\n",
    "    tau: float = field(default=20e-3)\n",
    "    gain: float = field(default=1.0)\n",
    "    bias: float = field(default=0.0)\n",
    "    f: Callable[[np.ndarray], np.ndarray] = field(default=relu)\n",
    "    x: float = field(default=0.0)\n",
    "\n",
    "\n",
    "    def reset(self, x0: float = 0.0) -> None:\n",
    "        self.x = float(x0)\n",
    "\n",
    "    def step(self, I_in: float) -> float:\n",
    "        \"\"\"\n",
    "        Advance one timestep with external input I_in.\n",
    "          x <- x + (dt/tau)*(-x + I_in)\n",
    "          r <- f(gain*x + bias)\n",
    "        \"\"\"\n",
    "        alpha = self.dt / self.tau\n",
    "        self.x = self.x + alpha * (-self.x + float(I_in))\n",
    "        r = float(self.f(np.array(self.gain * self.x + self.bias)))\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640bca1",
   "metadata": {},
   "source": [
    "## Spiking Neuron\n",
    "The spiking neuron is almost the same as the leaky rate neuron, but the output is an event (spike) of 0 or 1. This is a meaningful shift because we are representing information via timing so there is sparse outputs (more efficient), patterns in spiking times matter (temporal coding), and is a natural fit for event-driven systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4664a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LIFNeuron:\n",
    "    \"\"\"Single LIF spiking neuron (leaky integrator + threshold + reset).\n",
    "\n",
    "    Continuous-time form (current-based LIF):\n",
    "        dv/dt = (-(v - v_rest) + R * I(t)) / tau_m\n",
    "\n",
    "    Discrete-time Euler update:\n",
    "        v <- v + (dt/tau_m) * (-(v - v_rest) + R * I_in)\n",
    "\n",
    "    Spiking rule:\n",
    "        if v >= v_th: spike = 1, then v <- v_reset\n",
    "        optional refractory: hold at v_reset for a fixed time after spiking\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dt : float\n",
    "        Simulation time step in seconds.\n",
    "    tau_m : float\n",
    "        Membrane time constant in seconds. Larger => slower voltage dynamics.\n",
    "    v_rest : float\n",
    "        Resting voltage (baseline the voltage leaks toward).\n",
    "    v_reset : float\n",
    "        Voltage after a spike.\n",
    "    v_th : float\n",
    "        Spike threshold.\n",
    "    R : float\n",
    "        Input gain (often interpreted as membrane resistance). In toy units,\n",
    "        it just scales how strongly I_in pushes the voltage.\n",
    "    refractory : float\n",
    "        Refractory period in seconds (0 disables refractory behavior).\n",
    "    v : float\n",
    "        Current membrane voltage (internal state).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This class is network-ready: provide I_in from synapses, external stimulus,\n",
    "    noise, etc. The `step()` returns (spike, v).\n",
    "    \"\"\"\n",
    "\n",
    "    dt: float = 1e-3\n",
    "    tau_m: float = 20e-3\n",
    "    v_rest: float = 0.0\n",
    "    v_reset: float = 0.0\n",
    "    v_th: float = 1.0\n",
    "    R: float = 1.0\n",
    "    refractory: float = 5e-3\n",
    "\n",
    "    v: float = 0.0\n",
    "\n",
    "    # Derived / internal fields\n",
    "    _alpha: float = field(init=False, repr=False)\n",
    "    _refrac_steps: int = field(init=False, repr=False)\n",
    "    _refrac_count: int = field(init=False, repr=False, default=0)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.tau_m <= 0:\n",
    "            raise ValueError(\"tau_m must be > 0\")\n",
    "        if self.dt <= 0:\n",
    "            raise ValueError(\"dt must be > 0\")\n",
    "\n",
    "        self._alpha = self.dt / self.tau_m\n",
    "        self._refrac_steps = int(round(self.refractory / self.dt)) if self.refractory > 0 else 0\n",
    "        self._refrac_count = 0\n",
    "\n",
    "        # Default initial voltage: start at rest unless user already set v\n",
    "        # (If you prefer explicit, remove this line and require calling reset.)\n",
    "        if self.v == 0.0 and self.v_rest != 0.0:\n",
    "            self.v = float(self.v_rest)\n",
    "\n",
    "    def reset(self, v0: float | None = None) -> None:\n",
    "        \"\"\"Reset internal state (voltage and refractory counter).\"\"\"\n",
    "        self.v = float(self.v_rest if v0 is None else v0)\n",
    "        self._refrac_count = 0\n",
    "\n",
    "    def step(self, I_in: float) -> tuple[int, float]:\n",
    "        \"\"\"Advance one timestep with external input current/drive I_in.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        spike : int\n",
    "            1 if the neuron spiked this step, else 0.\n",
    "        v : float\n",
    "            The membrane voltage after the update (and after reset if spiked).\n",
    "        \"\"\"\n",
    "        # Refractory: hold at reset, do not integrate\n",
    "        if self._refrac_count > 0:\n",
    "            self._refrac_count -= 1\n",
    "            self.v = float(self.v_reset)\n",
    "            return 0, self.v\n",
    "\n",
    "        # Leak + integrate\n",
    "        self.v = self.v + self._alpha * (-(self.v - self.v_rest) + self.R * float(I_in))\n",
    "\n",
    "        # Threshold -> spike + reset\n",
    "        if self.v >= self.v_th:\n",
    "            self.v = float(self.v_reset)\n",
    "            if self._refrac_steps > 0:\n",
    "                self._refrac_count = self._refrac_steps\n",
    "            return 1, self.v\n",
    "\n",
    "        return 0, self.v\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".explorations (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
